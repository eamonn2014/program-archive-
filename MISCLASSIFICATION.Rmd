---
title:  "'Cutpoints made up by boys in back rooms'"
author: "Eamonn O'Brien"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document: default
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 8
    number_sections: yes
  word_document: default
---

### <span style="color:brown"> Introduction </span>

Clinical laboratory procedures are optimised to efficiently process patient samples in order 
to generate high quality patient information with low sample attrition. Yet the same is often not true in the analysis of laboratory data. Many machine learning techniques that are applied in this setting use improper accuracy scoring rules to dichotomise the output of laboratory processes, effectively throwing away precious patient samples. Optimisation of improper accuracy scoring rules will result in the wrong answer. Many clinicians too prefer 'classifying' patients, dichotomising continuous data and therefore patients, as 'high'/'low', 'positive'/'negative' etc, despite the loss of information and the detrimental impact to the patient. Information loss is greatest with only two groups, but this approach is most common. Cutpoints simply do not exist for continuous data as the cutpoint has to be a function of all other relevant covariates. Dichotomisation of clinical data is a truly disastrous practice, reducing statistical power, concealling non-linear relationships, is a colossal waste of time, money and resources; and yet it persists.  The statistican should not be the provider of the utility function, rather it is a job of the statistician to use all information as efficiently as possible and to deliver probabilities. A lack of statistical knowledge at all levels, including senior management is a major contributing factor to this issue. 

The following is an example of one problem that does not exist, yet is created by dichotomisation. An arbitrary cutpoint for an assay is used forcing a binary decision such that patients with results either side of the cutpoint are managed differently, perhaps 'positive' results (less than the cutpoint) are recruited on to a trial. A concern is raised regarding the 'misclassification' of patients that result from measurement error. 

The quote above is attributed to Frank Harrell Jr.
 

```{r set-options, echo=FALSE, cache=FALSE, warning = FALSE}

        rm(list=ls())

        set.seed(123)
        startTime<-proc.time()
        library(knitr)
        options(width=120)
        opts_chunk$set(comment = "", warning = FALSE, message = FALSE,
                       echo = TRUE, tidy = FALSE, size="tiny",  cache=FALSE,
                       progress=TRUE,
                       cache.path = 'program_Cache/',
                       fig.path='figure/')
         
        knitr::knit_hooks$set(inline = function(x) {
          knitr:::format_sci(x, 'md')
        })
```
```{r prerequisites, echo=FALSE}
 
        where<-"home" #this is used in the sourced program 
        
        path <- "RPUBS"  
     
        work<-    paste("X:/", path, sep = "")
        nonwork<- paste("~/X/", path, sep = "")
        if (where=="home") {wd<- nonwork} else {wd<-work}
        
        path2 <- "CUSTOMER\\SYROS\\CLIA VAIDATION RARalpha\\VACUFUGE\\DATA"  
        work2<-    paste("X:/", path2, sep = "")
        nonwork2<- paste("~/X/", path2, sep = "")
        
        if (where=="home") {wd2<- nonwork2} else {wd2<-work2}
        
        work3<-    paste("X:/FUNCTIONS/R", sep = "")
        nonwork3<- paste("~/X/FUNCTIONS/R", sep = "")
        
        if (where=="home") {wd3<- nonwork3} else {wd3<-work3}
        setwd(wd)
        opts_knit$set(root.dir = wd)                  ##THIS SETS YOUR WORKING DIRECTORY
```
 
 
```{r preliminaries perhaps , echo=FALSE, results='hide'}

     list.of.packages <- c("reshape" , "xlsx")
        
         new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
         if(length(new.packages)) install.packages(new.packages)

      #  require(pwpr)
        
 sapply(X = list.of.packages, require, character.only = TRUE)
        p3 <- function(x) {formatC(x, format="f", digits=3)}
        p4 <- function(x) {formatC(x, format="f", digits=4)}
        p2 <- function(x) {formatC(x, format="f", digits=2)}
         p1 <- function(x) {formatC(x, format="f", digits=0)}
        # p1 <- function(x) {print(formatC(x, format="f", digits=1),quote=FALSE)}
        # p2 <- function(x) {print(formatC(x, format="f", digits=2),quote=FALSE)}
        # p3 <- function(x) {print(formatC(x, format="f", digits=3),quote=FALSE)}
        # p4 <- function(x) {print(formatC(x, format="f", digits=4),quote=FALSE)}
        #perhaps help colour plot text based on loop count
        is.even <- function(x){ x %% 2 == 0 }
  
```            

##### <span style="color:brown"> Create continuous assay results for a number of reference patients </span>

```{r}

    set.seed(35252)        
    x <- rnorm(120, -1.81, 1.03)
    mosaic::favstats(x)

```

\clearpage
\pagebreak 

##### <span style="color:brown"> Convert patient responses to the probability of crossing the cutpoint due to normally distributed measurement error </span>


```{r echo=TRUE, eval=TRUE, tidy=FALSE, results='hide' }    

 
  threshold <- quantile(x, 0.25)[[1]]   # diff. patient management decisions made dependent on cutpoint
  noise <- 0.3                          # test method measurement error
  n <- length(x)       

  # convert the assay results to probabilities of flipping over the cutpoint 
  probx <- ifelse( x<threshold, pnorm(x, threshold, sd = noise), 
                              1-pnorm(x, threshold, sd = noise))

 
  
```

##### <span style="color:brown"> Number of samples with 25% or greater chance of changing call</span>

```{r echo=TRUE, eval=TRUE, tidy=FALSE, results='markup'}  
  
    binom::binom.confint( length(probx[probx>=.25]) , n, methods="wilson")
  
```

##### <span style="color:brown"> Probability distribution for different probabilities</span>


```{r echo=TRUE, eval=TRUE, tidy=FALSE, results='hide' }  

  miscall <- function(p) {
    
    Ex <- sum(p)           # expectation of the sum of the reference samples 
    var <- sum(p*(1-p))    # because the samples are independent this is the variance
    print(c(Ex, Ex + c(-1, +1)*qnorm(.975)*var^0.5))  # the distribution of sums is ~N(Ex, var)
    
  } 

 # counts and probabilites
 tot <- miscall(probx) / n                # FN + FP    
 FN <-  miscall(probx[x<threshold])  / n  # FN
 FP <-  miscall(probx[x>threshold])  / n  # FP

```

\clearpage
\pagebreak 


```{r echo=FALSE, eval=TRUE, tidy=TRUE, results='hide' }  

  plot(probx ~ x, lty=1, ylim=c(-0.01, 0.51),col="blue",
       xlab="Assay response", ylab="Probability of changing call", 
       main= paste("Reference samples N=",length(x),"\n Probability( miscall | SD, threshold ) on retest"), sep="")
  grid(NULL, NULL, lwd = 2) #
  abline(v=threshold)
  
  text(-.20 ,.5,  paste("FP ",p1(100*FP[1]),"% 95% CI (",p1(100*FP[2]), ", ",
                     p1(100*FP[3]),")", sep=""),  cex = .7)
  text(-0.18 ,.45,  paste("FN ",p1(100*FN[1]),"% 95% CI (",p1(100*FN[2]), ", ",
                     p1(100*FN[3]),")", sep=""), cex = .7)
  text(-.30 ,.4,  paste("FN + FP ",p1(100*tot[1]),"% 95% CI (",p1(100*tot[2]), ", ",
                      p1(100*tot[3]),")", sep="") ,cex = .7)
  text(-3.8 ,.48,  paste("Measurement error SD", noise, "units", sep=" ") ,cex = .7)
  text(-4.1 ,.45,  paste("Cutpoint", p2(threshold), "units", sep=" "), cex = .7)
  
```  
  
##### <span style="color:brown"> FN + FP counts and probabilities</span>
  
```{r echo=FALSE, eval=TRUE, tidy=FALSE  }    
  
   miscall(probx) / n

```  
  
##### <span style="color:brown"> FN counts and probabilities</span>
  
```{r echo=FALSE, eval=TRUE, tidy=FALSE  }     

   miscall(probx[x<threshold])  / n   

```  
  
##### <span style="color:brown"> FP counts and probabilities</span>
  
```{r echo=FALSE, eval=TRUE, tidy=FALSE  } 

   miscall(probx[x>threshold])  / n   
  
```

\clearpage
\pagebreak 

##### <span style="color:brown"> Convolution of probabilities</span>

```{r echo=TRUE, eval=TRUE, tidy=FALSE, results='hide' } 
  
  convolve.binomial <- function(p) {
    # p is a vector of probabilities of Bernoulli distributions.
    # The convolution of these distributions is returned as a vector
    # `z` where z[i] is the probability of i-1, i=1, 2, ..., length(p)+1.
    n <- length(p) + 1
    z <- c(1, rep(0, n-1))
    sapply(p, function(q) {z <<- (1-q)*z + q*(c(0, z[-n])); q})
    z
  }
  

  convolve <- function (p, user.text) {
    r<-convolve.binomial(p)
    r[1:20]
    ##cumulative
    x1<-r[1:20]
    names(x1) <- c(0:19)
    barplot(x1, las=1, main =paste("Probability of exactly x misclassifications :",user.text, sep=" "), 
            xlab="No of Misclassifications", ylab = "probability", col=rainbow(50))
  }
  
  
```

\clearpage
\pagebreak 

```{r echo=TRUE, eval=TRUE, tidy=TRUE, results='hide', fig.cap=""}    
  
  convolve(probx, user.text ="FN + FP")
  convolve(probx[x>threshold], user.text ="FP")
  convolve(probx[x<threshold], user.text ="FN")
  
   
  
```  

\clearpage
\pagebreak  
 
##### <span style="color:brown"> Simulation approach (an inefficient approximation to probability theory in this simple example).</span>
  
```{r echo=TRUE, eval=TRUE, tidy=FALSE, results='as.is', fig.cap=""}    

 
callum <- function(mdata=x, threshold,
                  noise, bias, 
                  seed=NULL){
  
  if (!is.null(seed)) set.seed(seed)
  
      n <- length(mdata)
      
      # split the sample and see what happens to them
      below <- mdata[mdata < threshold]
      above <- mdata[mdata > threshold]
      
      # analytical noise
      analytical1 <- rnorm(length(below), 0, noise) 
      analytical2 <- rnorm(length(above), 0, noise)
      
      # add the noise seperately and any bias
      a1 <- analytical1 + below + bias
      a2 <- analytical2 + above + bias
      
      prop <- length(below)/n
      
      # less than threshold are considered 'responders' - if you believe that
      (TP <- sum(a1 < threshold)/n) 
      (FP <- sum(a2 < threshold)/n)  
      (FN <- sum(a1 > threshold)/n)  
      (TN <- sum(a2 > threshold)/n)  
      FPFN <- FP+FN
      
        c(TN , FN, FP, FPFN, TP, prop, 1-prop)*100
      
      }
        
      res <- replicate(1e5, 
                callum(mdata=x,  
                           threshold=threshold, noise=noise, bias=0, seed=NULL)) 

      multi.fun <- function(x) {
            c(mean = mean(x), ci= quantile(x, c(0.025, 0.975)))
      }
      
      res1 <- apply(res, 1, multi.fun)  
      res1 <- as.data.frame(res1)
      names(res1) <- c("TN", "FN", "FP", "FN+FP", "TP", "Pos", "Neg")
      res1

      
```  
  
##### <span style="color:brown"> Print probability theory results again to cross check </span>
##### <span style="color:brown"> The first row below are the expected counts, the second row corresponds to the FN column above</span>
  
```{r echo=TRUE, eval=TRUE, tidy=FALSE  }   
      
      miscall(probx[x < threshold]) / n  # FN 
      
```

##### <span style="color:brown"> The second row below corresponds to the FP column above</span>
  
```{r echo=TRUE, eval=TRUE, tidy=FALSE  }   
      
      miscall(probx[x > threshold]) / n  # FP

```

##### <span style="color:brown"> The second row below corresponds to the FN+FP column above</span>
  
```{r echo=TRUE, eval=TRUE, tidy=FALSE  }   
      
      miscall(probx) / n                 # FN + FP
      
```

\clearpage
\pagebreak 

##### <span style="color:brown"> Reference </span>

   http://stats.stackexchange.com/questions/41247/risk-of-extinction-of-schr%c3%b6dingers-cats/41263#41263      
        
   http://stats.stackexchange.com/questions/9510/probability-distribution-for-different-probabilities    
   
   https://www.youtube.com/watch?v=uULhuuSjBww
   
   https://www.youtube.com/watch?v=twnqtGCCTVE
   
   The function 'callum' is named after Professor Callum George Fraser
 

\clearpage
\pagebreak 


##### <span style="color:brown"> Computing Environment</span>

```{r, echo=FALSE}
#opts_knit$set(root.dir = wd)   ##THIS SETS YOUR WORKING DIRECTORY
sessionInfo()
print(wd)
```
```{r echo=FALSE}
stopTime<-proc.time()
```
This took `r (stopTime-startTime)[1][[1]]` seconds to execute. 
  

